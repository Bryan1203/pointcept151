{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "import os\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_semantic_kitti(dataset_path, pred_path, sequence, start_index=0, end_index=None, include_labels=True, include_predictions=True):\n",
    "    \"\"\"\n",
    "    Process SemanticKITTI-like dataset, optionally including labels and predictions.\n",
    "    Args:\n",
    "    dataset_path (str): Path to the dataset\n",
    "    sequence (str): Sequence number\n",
    "    start_index (int): Index of the first scan to process (default: 0)\n",
    "    end_index (int): Index of the last scan to process (exclusive). If None, process until the end.\n",
    "    include_labels (bool): Whether to include ground truth labels\n",
    "    include_predictions (bool): Whether to include prediction labels\n",
    "    Returns:\n",
    "    dict: A dictionary containing processed data\n",
    "    \"\"\"\n",
    "    # Initialize empty lists to store all data\n",
    "    all_coords = []\n",
    "    all_intensity = []\n",
    "    all_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    # Get all .bin files in the sequence\n",
    "    bin_files = glob.glob(os.path.join(dataset_path, \"sequences\", sequence, \"velodyne_orig\", \"*.bin\"))\n",
    "    bin_files.sort()\n",
    "\n",
    "    # Apply start and end index\n",
    "    bin_files = bin_files[start_index:end_index]\n",
    "\n",
    "    for points_file in bin_files:\n",
    "        # Extract the frame number from the file name\n",
    "        frame = os.path.basename(points_file).split('.')[0]\n",
    "        \n",
    "        # Load the point cloud data from the .bin file\n",
    "        points = np.fromfile(points_file, dtype=np.float32).reshape(-1, 4)\n",
    "        \n",
    "        # Extract the coordinates and intensity from the point cloud data\n",
    "        coords = points[:, :3]\n",
    "        intensity = points[:, 3]\n",
    "        \n",
    "        # Append the data to the respective lists\n",
    "        all_coords.append(coords)\n",
    "        all_intensity.append(intensity)\n",
    "        \n",
    "        if include_predictions:\n",
    "            # Load the predicted segmentation labels from the .label file\n",
    "            pred_file = os.path.join(pred_path, \"sequences\", sequence, \"predictions\", \"{0}.label\".format(frame))\n",
    "            if os.path.exists(pred_file):\n",
    "                pred_labels = np.fromfile(pred_file, dtype=np.uint32)\n",
    "                # Extract the lower 16 bits of pred_labels\n",
    "                pred_labels = pred_labels & 0xFFFF\n",
    "                all_pred_labels.append(pred_labels)\n",
    "            else:\n",
    "                print(\"Warning: Prediction file not found for frame {0}\".format(frame))\n",
    "        \n",
    "        if include_labels:\n",
    "            # Load the ground truth labels from the .label file\n",
    "            label_file = os.path.join(dataset_path, \"sequences\", sequence, \"labels\", \"{0}.label\".format(frame))\n",
    "            if os.path.exists(label_file):\n",
    "                labels = np.fromfile(label_file, dtype=np.uint32)\n",
    "                # Extract the lower 16 bits of labels\n",
    "                labels = labels & 0xFFFF\n",
    "                all_labels.append(labels)\n",
    "            else:\n",
    "                print(\"Warning: Label file not found for frame {0}\".format(frame))\n",
    "\n",
    "    # Concatenate all data into single arrays\n",
    "    result = {\n",
    "        'xyz': np.concatenate(all_coords),\n",
    "        'intensity': np.concatenate(all_intensity)\n",
    "    }\n",
    "\n",
    "    if include_predictions and all_pred_labels:\n",
    "        result['predictions'] = np.concatenate(all_pred_labels)\n",
    "    \n",
    "    if include_labels and all_labels:\n",
    "        result['labels'] = np.concatenate(all_labels)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.spatial import cKDTree\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_segment_directions(points):\n",
    "    diffs = points[1:] - points[:-1]\n",
    "    return np.arctan2(diffs[:, 1], diffs[:, 0])\n",
    "\n",
    "def direction_based_clustering(points, eps, min_samples, direction_weight=0.5):\n",
    "    n_points = len(points)\n",
    "    directions = compute_segment_directions(points)\n",
    "    directions = np.append(directions, directions[-1])\n",
    "    \n",
    "    tree = cKDTree(points[:, :2])\n",
    "    neighbors = tree.query_ball_tree(tree, eps)\n",
    "    \n",
    "    clusters = np.full(n_points, -1, dtype=np.int32)\n",
    "    cluster_id = 0\n",
    "    \n",
    "    for i in tqdm(xrange(n_points), desc=\"Clustering\"):\n",
    "        if clusters[i] != -1:\n",
    "            continue\n",
    "        \n",
    "        if len(neighbors[i]) < min_samples:\n",
    "            continue\n",
    "        \n",
    "        dir_diffs = np.abs(directions[i] - directions[neighbors[i]])\n",
    "        dir_diffs = np.minimum(dir_diffs, 2*np.pi - dir_diffs)\n",
    "        \n",
    "        dists = np.sqrt(np.sum((points[neighbors[i], :2] - points[i, :2])**2, axis=1))\n",
    "        combined_dists = dists + direction_weight * dir_diffs\n",
    "        \n",
    "        core_neighbors = np.array(neighbors[i])[combined_dists <= eps]\n",
    "        \n",
    "        if len(core_neighbors) < min_samples:\n",
    "            continue\n",
    "        \n",
    "        clusters[i] = cluster_id\n",
    "        stack = [i]\n",
    "        \n",
    "        while stack:\n",
    "            current = stack.pop()\n",
    "            for neigh in neighbors[current]:\n",
    "                if clusters[neigh] == -1:\n",
    "                    dir_diff = min(abs(directions[current] - directions[neigh]), 2*np.pi - abs(directions[current] - directions[neigh]))\n",
    "                    dist = np.sqrt(np.sum((points[neigh, :2] - points[current, :2])**2))\n",
    "                    combined_dist = dist + direction_weight * dir_diff\n",
    "                    \n",
    "                    if combined_dist <= eps:\n",
    "                        clusters[neigh] = cluster_id\n",
    "                        stack.append(neigh)\n",
    "        \n",
    "        cluster_id += 1\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def fit_line(x, y, z, points_per_unit=1, degree=1):\n",
    "    sorted_indices = np.argsort(x)\n",
    "    x, y, z = x[sorted_indices], y[sorted_indices], z[sorted_indices]\n",
    "\n",
    "    total_length = np.sum(np.sqrt(np.diff(x)**2 + np.diff(y)**2))\n",
    "    num_points = int(np.ceil(total_length * points_per_unit))\n",
    "\n",
    "    X = np.column_stack((x, y))\n",
    "    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    \n",
    "    model = make_pipeline(poly_features, LinearRegression())\n",
    "    model.fit(X, z)\n",
    "    \n",
    "    t = np.linspace(0, 1, num_points)\n",
    "    x_line = np.interp(t, np.linspace(0, 1, len(x)), x)\n",
    "    y_line = np.interp(t, np.linspace(0, 1, len(y)), y)\n",
    "    X_line = np.column_stack((x_line, y_line))\n",
    "    \n",
    "    z_line = model.predict(X_line)\n",
    "    \n",
    "    return x_line, y_line, z_line\n",
    "\n",
    "def extract_road_lines(data, road_line_labels, eps=0.5, min_samples=5, points_per_unit=1, degree=1, direction_weight=0.5):\n",
    "    road_lines = []\n",
    "    \n",
    "    for label in tqdm(road_line_labels, desc=\"Processing road lines\"):\n",
    "        road_mask = data['predictions'] == label\n",
    "        road_points = data['xyz'][road_mask]\n",
    "        \n",
    "        if len(road_points) < min_samples:\n",
    "            print(\"Warning: Not enough points for road line label %d. Skipping.\" % label)\n",
    "            continue\n",
    "        \n",
    "        cluster_labels = direction_based_clustering(road_points, eps, min_samples, direction_weight)\n",
    "        \n",
    "        unique_clusters = np.unique(cluster_labels)\n",
    "        unique_clusters = unique_clusters[unique_clusters != -1]  # Remove noise points\n",
    "        \n",
    "        for cluster_label in unique_clusters:\n",
    "            cluster = road_points[cluster_labels == cluster_label]\n",
    "            \n",
    "            if len(cluster) < min_samples:\n",
    "                continue\n",
    "            \n",
    "            x, y, z = cluster[:, 0], cluster[:, 1], cluster[:, 2]\n",
    "            \n",
    "            try:\n",
    "                x_line, y_line, z_line = fit_line(x, y, z, points_per_unit, degree)\n",
    "                road_lines.append((label, np.column_stack((x_line, y_line, z_line))))\n",
    "            except Exception as e:\n",
    "                print(\"Error processing cluster for label %d: %s\" % (label, str(e)))\n",
    "                continue\n",
    "    \n",
    "    if not road_lines:\n",
    "        print(\"Warning: No valid road lines were extracted.\")\n",
    "        return np.array([], dtype=[('label', int), ('points', object)])\n",
    "    \n",
    "    return np.array(road_lines, dtype=[('label', int), ('points', object)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "dataset_path = r\"F:\\itriDataset\\nanliao_taoyuan_hct\\dataset\"\n",
    "pred_path = r\"F:\\itriDataset\\exp\\test_itri_with_nanliao_taoyuan_hct_tai61_weight0727\\result\\submit\"\n",
    "sequence = \"09\"  # or whichever sequence you're processing\n",
    "road_line_labels = [1, 2, 3]  # replace with your actual labels for different road line types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing road lines:   0%|                                                                     | 0/3 [00:00<?, ?it/s]\n",
      "Clustering:   0%|                                                                            | 0/66998 [00:00<?, ?it/s]\u001b[A\n",
      "Clustering:   0%|                                                                | 1/66998 [00:36<673:16:57, 36.18s/it]\u001b[A\n",
      "Clustering:   1%|▌                                                               | 652/66998 [00:46<1:00:13, 18.36it/s]\u001b[A\n",
      "Clustering:  37%|███████████████████████▏                                       | 24703/66998 [00:46<00:43, 982.35it/s]\u001b[A\n",
      "Clustering:  73%|█████████████████████████████████████████████                 | 48754/66998 [00:57<00:12, 1421.57it/s]\u001b[A\n",
      "Clustering:  78%|████████████████████████████████████████████████▎             | 52228/66998 [00:58<00:09, 1567.90it/s]\u001b[A\n",
      "Clustering: 100%|██████████████████████████████████████████████████████████████| 66998/66998 [00:58<00:00, 1140.18it/s]\u001b[A\n",
      "Processing road lines:  33%|████████████████████▎                                        | 1/3 [01:22<02:44, 82.17s/it]\n",
      "Clustering:   0%|                                                                           | 0/186038 [00:00<?, ?it/s]\u001b[A\n",
      "Clustering:   0%|                                                                | 63/186038 [00:01<1:00:54, 50.89it/s]\u001b[A\n",
      "Clustering:   1%|▋                                                              | 1983/186038 [00:02<03:08, 974.45it/s]\u001b[A\n",
      "Clustering:   1%|▋                                                              | 2084/186038 [00:10<20:07, 152.36it/s]\u001b[A\n",
      "Clustering:   4%|██▋                                                            | 7853/186038 [00:16<05:24, 548.39it/s]\u001b[A\n",
      "Clustering:  11%|██████▍                                                      | 19634/186038 [00:16<01:32, 1794.06it/s]\u001b[A\n",
      "Clustering:  27%|████████████████▌                                            | 50561/186038 [00:22<00:37, 3640.99it/s]\u001b[A\n",
      "Clustering:  28%|████████████████▊                                            | 51315/186038 [00:23<00:40, 3337.88it/s]\u001b[A\n",
      "Clustering:  28%|█████████████████                                            | 51855/186038 [00:30<01:24, 1581.39it/s]\u001b[A\n",
      "Clustering:  28%|█████████████████▏                                           | 52594/186038 [00:30<01:21, 1638.37it/s]\u001b[A\n",
      "Clustering:  30%|██████████████████                                           | 54929/186038 [00:31<01:14, 1751.67it/s]\u001b[A\n",
      "Clustering:  31%|██████████████████▋                                          | 57052/186038 [00:32<01:12, 1779.00it/s]\u001b[A\n",
      "Clustering:  40%|████████████████████████▏                                    | 73761/186038 [00:32<00:21, 5128.83it/s]\u001b[A\n",
      "Clustering:  43%|██████████████████████████▎                                  | 80355/186038 [00:33<00:15, 6660.62it/s]\u001b[A\n",
      "Clustering:  44%|███████████████████████████▏                                 | 82785/186038 [00:33<00:15, 6830.64it/s]\u001b[A\n",
      "Clustering:  46%|███████████████████████████▊                                 | 84732/186038 [00:33<00:15, 6711.70it/s]\u001b[A\n",
      "Clustering:  48%|█████████████████████████████▍                               | 89692/186038 [00:34<00:13, 7392.53it/s]\u001b[A\n",
      "Clustering:  52%|███████████████████████████████▌                             | 96427/186038 [00:35<00:13, 6842.79it/s]\u001b[A\n",
      "Clustering:  55%|████████████████████████████████▉                           | 102103/186038 [00:35<00:09, 8787.42it/s]\u001b[A\n",
      "Clustering:  60%|███████████████████████████████████▎                       | 111473/186038 [00:35<00:05, 13715.44it/s]\u001b[A\n",
      "Clustering:  63%|█████████████████████████████████████▌                      | 116368/186038 [00:36<00:07, 9710.89it/s]\u001b[A\n",
      "Clustering:  64%|██████████████████████████████████████▎                     | 118749/186038 [00:36<00:06, 9914.89it/s]\u001b[A\n",
      "Clustering:  67%|███████████████████████████████████████▎                   | 124036/186038 [00:37<00:05, 11454.18it/s]\u001b[A\n",
      "Clustering:  68%|███████████████████████████████████████▉                   | 125814/186038 [00:37<00:05, 11632.53it/s]\u001b[A\n",
      "Clustering:  70%|█████████████████████████████████████████                  | 129310/186038 [00:37<00:04, 13369.93it/s]\u001b[A\n",
      "Clustering:  70%|█████████████████████████████████████████▌                 | 131131/186038 [00:37<00:04, 11800.90it/s]\u001b[A\n",
      "Clustering:  71%|██████████████████████████████████████████                 | 132628/186038 [00:37<00:05, 10404.25it/s]\u001b[A\n",
      "Clustering:  72%|███████████████████████████████████████████▍                | 134753/186038 [00:38<00:06, 7945.96it/s]\u001b[A\n",
      "Clustering:  74%|███████████████████████████████████████████▊               | 138068/186038 [00:38<00:04, 10557.87it/s]\u001b[A\n",
      "Clustering:  75%|█████████████████████████████████████████████               | 139552/186038 [00:38<00:04, 9565.87it/s]\u001b[A\n",
      "Clustering:  76%|█████████████████████████████████████████████▍              | 140794/186038 [00:38<00:04, 9924.89it/s]\u001b[A\n",
      "Clustering:  77%|█████████████████████████████████████████████▉              | 142416/186038 [00:39<00:06, 6880.15it/s]\u001b[A\n",
      "Clustering:  77%|██████████████████████████████████████████████▏             | 143384/186038 [00:39<00:09, 4386.88it/s]\u001b[A\n",
      "Clustering:  80%|███████████████████████████████████████████████▊            | 148281/186038 [00:40<00:04, 8983.89it/s]\u001b[A\n",
      "Clustering:  81%|████████████████████████████████████████████████▍           | 150259/186038 [00:40<00:05, 6822.47it/s]\u001b[A\n",
      "Clustering:  84%|█████████████████████████████████████████████████▎         | 155447/186038 [00:40<00:02, 11636.47it/s]\u001b[A\n",
      "Clustering:  85%|██████████████████████████████████████████████████         | 158010/186038 [00:40<00:02, 12184.72it/s]\u001b[A\n",
      "Clustering:  86%|██████████████████████████████████████████████████▉        | 160595/186038 [00:40<00:01, 14164.06it/s]\u001b[A\n",
      "Clustering:  88%|███████████████████████████████████████████████████▋       | 162916/186038 [00:41<00:01, 15100.52it/s]\u001b[A\n",
      "Clustering:  89%|█████████████████████████████████████████████████████▌      | 165900/186038 [00:42<00:03, 5844.56it/s]\u001b[A\n",
      "Clustering:  90%|██████████████████████████████████████████████████████      | 167507/186038 [00:45<00:10, 1776.13it/s]\u001b[A\n",
      "Clustering:  95%|█████████████████████████████████████████████████████████▎  | 177578/186038 [00:45<00:01, 4759.19it/s]\u001b[A\n",
      "Clustering: 100%|████████████████████████████████████████████████████████████| 186038/186038 [00:45<00:00, 4049.06it/s]\u001b[A\n",
      "Processing road lines:  67%|████████████████████████████████████████▋                    | 2/3 [02:27<01:12, 72.05s/it]\n",
      "Clustering:   0%|                                                                            | 0/22650 [00:00<?, ?it/s]\u001b[A\n",
      "Clustering:   0%|▏                                                                | 66/22650 [00:12<1:11:50,  5.24it/s]\u001b[A\n",
      "Clustering: 100%|██████████████████████████████████████████████████████████████| 22650/22650 [00:14<00:00, 1589.25it/s]\u001b[A\n",
      "Processing road lines: 100%|█████████████████████████████████████████████████████████████| 3/3 [02:50<00:00, 56.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process the data\n",
    "data = process_semantic_kitti(dataset_path, dataset_path, sequence, start_index=0, end_index=150, include_labels=True, include_predictions=True)\n",
    "\n",
    "# Extract road lines\n",
    "#road_lines = extract_road_lines(data, road_line_labels, eps=1.0, min_samples=100, points_per_unit=0.1, degree=2)\n",
    "road_lines = extract_road_lines(data, road_line_labels, eps=1.0, min_samples=100, points_per_unit=0.1, degree=2, direction_weight=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_offset = 1.0\n",
    "points = data['xyz']\n",
    "colors = np.zeros_like(points)\n",
    "intensities = data['intensity']\n",
    "\n",
    "# Define colors for each road line type\n",
    "unique_labels = np.unique([line[0] for line in road_lines])\n",
    "line_colors = {\n",
    "    unique_labels[0]: [1, 0, 0],  # Red for first type\n",
    "    unique_labels[1]: [0, 1, 0],  # Green for second type\n",
    "    unique_labels[2]: [0, 0, 1]   # Blue for third type\n",
    "}\n",
    "\n",
    "# Create a color lookup array\n",
    "color_lookup = np.array([line_colors[label] for label in unique_labels])\n",
    "\n",
    "# Stack all road line points\n",
    "all_line_points = np.vstack([line[1] for line in road_lines])\n",
    "\n",
    "# Create offset for all points at once\n",
    "offset_lines = all_line_points + np.array([0, 0, z_offset])\n",
    "\n",
    "# Stack points\n",
    "points = np.vstack((points, offset_lines))\n",
    "\n",
    "# Create colors for all new points\n",
    "labels = np.concatenate([np.full(len(line[1]), line[0]) for line in road_lines])\n",
    "new_colors = color_lookup[np.searchsorted(unique_labels, labels)]\n",
    "colors = np.vstack((colors, new_colors))\n",
    "\n",
    "# Add dummy intensity values for the new points\n",
    "dummy_intensity = np.full(len(all_line_points), intensities.mean())\n",
    "intensities = np.hstack((intensities, dummy_intensity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create viewer\n",
    "v = pptk.viewer(points)\n",
    "\n",
    "# Set point colors\n",
    "v.attributes(intensities, colors)\n",
    "\n",
    "# Set point size (smaller for road lines)\n",
    "v.set(point_size=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create color map\n",
    "color_map = np.zeros((np.max(48) + 1, 3))\n",
    "# White lines (1-6)\n",
    "color_map[1] = [0.5, 0.5, 0.5]  # broken\n",
    "color_map[2] = [1.0, 1.0, 1.0]  # solid \n",
    "color_map[3] = [0.8, 0.8, 0.0]  # solid solid\n",
    "\n",
    "# color_map[4] = [0.8, 0.8, 0.0]  # solid broken\n",
    "# color_map[5] = [0.6, 0.6, 0.6]  # broken solid\n",
    "# color_map[6] = [0.5, 0.5, 0.5]  # broken broken\n",
    "\n",
    "# # Yellow lines (11-16)\n",
    "# color_map[11] = [1.0, 1.0, 0.0]  # solid\n",
    "# color_map[12] = [0.9, 0.9, 0.0]  # broken\n",
    "# color_map[13] = [0.8, 0.8, 0.0]  # solid solid\n",
    "# color_map[14] = [0.7, 0.7, 0.0]  # solid broken\n",
    "# color_map[15] = [0.6, 0.6, 0.0]  # broken solid\n",
    "# color_map[16] = [0.5, 0.5, 0.0]  # broken broken\n",
    "\n",
    "# # Red lines (21-26)\n",
    "# color_map[21] = [1.0, 0.0, 0.0]  # solid\n",
    "# color_map[22] = [0.9, 0.0, 0.0]  # broken\n",
    "# color_map[23] = [0.8, 0.0, 0.0]  # solid solid\n",
    "# color_map[24] = [0.7, 0.0, 0.0]  # solid broken\n",
    "# color_map[25] = [0.6, 0.0, 0.0]  # broken solid\n",
    "# color_map[26] = [0.5, 0.0, 0.0]  # broken broken\n",
    "\n",
    "color_map[41] = [0.5, 0.5, 0.5]  # white broken\n",
    "color_map[42] = [1.0, 1.0, 1.0]  # white solid\n",
    "color_map[43] = [0.8, 0.8, 0.0]  # white solid solid\n",
    "color_map[44] = [0.9, 0.9, 0.9]  # yellow broken\n",
    "color_map[45] = [1.0, 1.0, 1.0]  # yellow solid\n",
    "color_map[46] = [0.8, 0.8, 0.0]  # yellow solid solid\n",
    "color_map[47] = [1.0, 1.0, 1.0]  # red solid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# dataset_path = r\"F:\\itriDataset\\nanliao_taoyuan_hct\\dataset\"\n",
    "# pred_path = r\"F:\\itriDataset\\exp\\test_itri_with_nanliao_taoyuan_hct_tai61_weight0727\\result\\submit\"\n",
    "# sequence = \"09\"\n",
    "\n",
    "# # Process with both labels and predictions, limiting to the first 5 scans\n",
    "# data = process_semantic_kitti(dataset_path, dataset_path, sequence, start_index=0, end_index=150, include_labels=True, include_predictions=True)\n",
    "\n",
    "# Create a point cloud viewer using pptk\n",
    "#viewer = pptk.viewer(data['coordinates'], data['intensity'], data['labels'],data['predictions'])\n",
    "road_color = np.zeros((len(data['xyz']),3))\n",
    "road_color = color_map[data['labels']]\n",
    "\n",
    "road_color_l = np.zeros((len(data['xyz']),3))\n",
    "road_color_l = color_map[data['predictions']]\n",
    "\n",
    "viewer = pptk.viewer(data['xyz'], 255.*data['intensity'],road_color, road_color_l)\n",
    "\n",
    "viewer.color_map('jet', [0,70])\n",
    "viewer.set(point_size=0.002)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
